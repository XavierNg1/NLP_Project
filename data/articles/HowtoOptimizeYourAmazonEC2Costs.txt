 In terms of price-performance ratios for the hardware alone, EC2 is far from the cheapest alternative on the market. Even accounting for reliability of service does not make it the most cost-competitive.If you expect to have a lot of data transfer (outbound from your server) the Amazon Web Services can be quite expensive compared with various virtual private server (VPS) providers such as Linode and Digital Ocean. This is because AWS charges about 9 cents per GB, which translates to about $90/TB, an amount that most VPS providers offer for free with their minimalistic monthly plans (about $5 or $10 per month).In other words, if you have a simple website that gets a lot of traffic, AWS is unlikely to be the right choice for you.
In terms of hardware performance, Amazon EC2 has historically offered worse performance than Linode and Digital Ocean for servers of the same price and similar hardware specs (in terms of vCPUs and memory). Partly, this is because of differences in the underlying architecture.;
,
Ability to deploy instances and make changes to the architecture programmatically.
The availability of spot instances.
A large number of managed services that, if used together in the same region, cost nothing (or very little) in data transfer.
 For instance, a number of people use Amazon S3 for cheap, flexible, and redundant bulk storage, but do not run their machines on EC2., For instance, "r3.4xlarge" is instance type R, generation 3, and size 4xlarge.
A simple way of remembering what the size means is: "large" stands for 2 vCPUs, "xlarge" stands for 4 vCPUs, and nxlarge means 4n vCPUs. A vCPU is one "hyperthread" in the jargon of Intel, the chip manufacturer.It can naively be thought of as corresponding to one core on a consumer laptop or desktop; however, physically speaking, the Intel chips used by recent generations of EC2 instances have two vCPUs (or two threads) per core. If comparing to existing physical servers, you have to multiply the number of physical server cores by two to get the right vCPU number.The instance class gives the ratio between the different parts of the instance specs. The most relevant ratio is the ratio of vCPUs to RAM. For instance, the C instance class (where C stands for compute-optimized) offers 1 vCPU for every (approximately) 2 gigabytes of RAM. The exact ratios differ slightly between different generations, since later instances do a better job squeezing out more value from the hardware.
Generations also differ in some of the extra features they offer. For instance, the third-generation C, M, and R classes (C3, M3 and R3) all have local SSDs, but the fourth-generation (C4, M4, and R4) do not.
For a given instance class and generation, size differences just mean different amounts of each resource, but in the same proportion (note that some peripheral aspects of the specs, such as SSD storage and throughput, do not scale linearly). For on-demand and reserved instances, costs scale linearly with size within a given instance type and generation. For spot instance, costs may not scale linearly since they are determined by supply and demand, but for the most common instance types, the scaling is close to linear.
For a given instance type and generation, it is generally possible to change a reservation type (after the reservation has already been made) to reallocate capacity between different sizes. For instance, c3.2xlarge is twice the capacity of c3.xlarge, so it is possible to change a reservation of 5 c3.2xlarge's into 10 c3.xlarge's, or into 3 c3.2xlarge's and 4 c3.xlarge's.
Keep in mind that the names of the instance types don't have any deeper meaning than just providing an intuitive description of the specs. Thus, for instance, C is "compute-optimized" but all this means is that the ratio of vCPUs to memory is more in favor of vCPUs than in memory. There is no specific computation-specific optimization beyond what the specs already reveal.
Network throughput does not quite scale linearly.
 Note that the precise ratios vary a bit across generations.

The R instance class is memory-optimized, and offers the most memory per vCPU (i.e., the least number of vCPUs per unit memory). The ratio is approximately 7.5 GB/vCPU.
The M instance class is intermediate. It offers 3.75 GB/vCPU.
The C instance class is compute-optimized, and offers the least memory per vCPU (i.e., the most number of vCPUs per unit memory). The ratio is approximately 1.875 GB/vCPU.


M instance class: M3 goes up to only m3.2xlarge (30 GB, 8 vCPUs). M4 goes up to m4.16xlarge (256 GB, 64 vCPUs) but lacks SSD.
R instance class: R3 goes up to r3.8xlarge (244 GB, 32 vCPUs). R4 goes up to r4.16xlarge (488 GB, 64 vCPUs) but lacks SSD.
C instance class: C3 goes up to c3.8xlarge (60 GB, 32 vCPUs). C4 goes up to c4.8xlarge (60 GB, 36 vCPUs) but lacks SSD. C5 (which is being rolled out as of December 2016) will go up to c5.18xlarge (144 GB, 72 vCPUs) and also lack SSD.


Most of the remarks in this article, as well as most online discussion of EC2, focuses on the use case of Linux/Unix instances that have no licensing costs.
You can also deploy EC2 instances with other operating systems such as Windows. These instances cost more (holding the instance type and purchase option constant). They also offer less flexibility with changing reservations. There are no separate licensing fees; Amazon pays for the licenses and includes them in the instance costs.,

CPU and network usage are stored in metrics in Amazon CloudWatch, Amazon's system for recording metrics. They can also be accessed in the EC2 console.
Memory usage is not trackable using the Amazon EC2 console. Therefore you will need to track memory usage within your application, or through another memory-logging process that you install on your instance. One such process recommended by Amazon (and that can export to CloudWatch) is collectd.Keep in mind that CPU and network usage data are no longer available in the EC2 console after your instances are terminated. However, they can be viewed in CloudWatch metrics (essentially the reason you can't see them in the EC2 console is that the instance isn't listed there any more).
CloudWatch metrics on CPU and network usage (as well as any other custom metrics that you export) are maintained for a moving window of 15 months, up from a moving window of 2 weeks earlier. Since the change was introduced recently, as of now, you can only get the metrics for the last three months.,

For frontend applications, one key variable affecting resource usage is traffic levels. Identify how your resource usage (both memory and computational resources) varies with different traffic levels. Traffic levels may fluctuate daily and seasonally as well as have secular trends (i.e., long-term trends). You may wish to artificially simulate higher traffic loads using tools such as Gatlingor services such as Blitz.io.
The size of the data that your application uses may also change, independent of traffic levels. For instance, if your application serves a website, then metrics related to the size of the website (number of pages, number of distinct user accounts) may affect resource usage. These metrics do not vary much over the short term but tend to increase over time, so you will need to extrapolate from current usage or artificially simulate larger website size or more user accounts.


For applications that run on the Java Virtual Machine (JVM), the closer your memory is to full utilization, the more time and resources are spent in garbage collection.This can cause CPU utilization to skyrocket and latency to increase. Similar phenomena may occur for applications that run in other environments.
Therefore, it's particularly important to track and understand what the original cause of bottlenecks is. Simply seeing CPU utilization skyrocket to 100% does not imply that the problem was with too little CPU. The problem could be with too little memory causing CPU resources to be forced into garbage collection.
 For instance, determine whether it's better to use a few xlarge instances, or twice as many large instances.Limits (in favor of horizontal scaling): Vertical scaling has fairly tight limits: there is a fairly low upper bound on the size of EC2 instances you can use (see Part 2, Step 3). With AWS's infrastructure scale, the limits on horizontal scaling are much larger (although your account might have its own limits set by AWS, you can request a limit increase). If you need 1000 vCPUs of computation, you must use at least some horizontal scaling, because even the limits of vertical scaling only get you to 64 vCPUs.

More divisibility and therefore better precision in capacity (in favor of horizontal scaling): Using smaller instance types allows you to more finely tune the number of instances to the traffic capacity. For instance, suppose you know your traffic requirement would need 9 c3.large instances to serve. Assuming no shared memory or other shared resource issues, If you wanted to use c3.xlarge instances, you'd need 5 of them, because you can't get 4.5 instances, therefore effectively wasting the equivalent of one c3.large in computational resources. If you used c3.2xlarge instances, you'd need 3 of them, thereby effectively wasting the equivalent of three c3.large's in computational resources. If you used c3.4xlarge's you'd need 2 of them, thereby effectively wasting the equivalent of seven c3.large's. Note that this applies both if you have very fixed traffic needs and if you have variable traffic needs but have a good system for autoscaling.

Improved availability (mixed, but generally in favor of horizontal scaling): Horizontal scaling allows for more availability because if any one instance goes down, your capacity is temporarily reduced only a little bit. In contrast, with vertical scaling, any single instance going down hurts capacity a lot. On the other hand, horizontal scaling can reduce availability if the instances, being small, have less of a buffer to handle a single computationally intensive request, and temporarily go down on receiving such a request.

Cost stability (mixed, but generally in favor of horizontal scaling): For spot instances in particular, costs are more stable for smaller instances because of the larger number of people who use them. However, this is not uniformly true.

Shared memory (in favor of vertical scaling): If your application uses a lot of common in-memory data to process requests, then vertical scaling is better since it allows for the in-memory data to be shared. For instance, if you are providing a search engine and you store all the indexes in RAM, and the indexes use up 6 GB of data. If you use two m3.large instances, you are duplicating the 6 GB across both machines, and have only 1.5 GB (= 7.5 - 6) left for doing computation on each instance. On the other hand, if you use one m3.2xlarge, you have an effective 9 GB of memory left for doing computation. Even if you don't store all the data in-memory, but query it from a data store, shared memory can still help you by allowing you to cache resources. Note that the shared memory consideration is also relevant to deciding between instance classes, e.g., determining whether M or C makes more sense.
 AWS regions are names given to clusters of geographically nearby Amazon Web Services data centers. As of December 2016, there are twelve AWS regions (excluding AWS GovCloud):four in the United States, five in Asia-Pacific, two in Europe, and one in South America. Additional AWS regions are expected to be added in Europe soon.Round-trip times within an AWS region are approximately 2 milliseconds.
Data transfer between different AWS services within a region, including to and from EC2 instances, is substantially cheaper than cross-region data transfer but not completely free.
Prices differ by AWS region, but are the same within a given AWS region.
 The number of AZs per region vary from 2 to 4.
The AZs are all isolated from each other, so that failures in one AZ (such as fires or electricity blackouts) should not adversely affect the operation of the other AZ.
The AZ for an EC2 instance is specified at the time of instance creation.
While the prices for on-demand and reserved instances are the same across different availability zones in a region, the spot instance markets are different for the different availability zones.
Reservations used to be tied to a particular availability zone. Starting September 2016, reservations can be given availability zone scope or region scope. If given region scope, the reservation is not tied to an availability zone.Instances reserved prior to that have availability zone scope but can be changed to region scope.
 A given EBS can be attached to at most one EC2 instance at a time, but the instance to which it is attached can be changed. EBS can persist even when the instance is stopped and (if specified at launch) even after the instance is terminated.
Instance storage is local storage associated with a particular instance. It offers faster input/output but no redundancy and no persistence.
Depending on the Amazon Machine Image (AMI) used when launching an instance, the root volume of the instance may be either an EBS store or an instance store. The former types of instances are called EBS boot instances or EBS-backed instances.
New generation instances (C4, M4, R4, and C5) do not offer instance storage. They only support EBS.

EBS also charges for I/O. There are a few different kinds of EBS with different pricing models. For usual EBS, I/O charges occur whenever I/O occurs. For gp2, which is designed for high throughput, you are charged for provisioned throughput, rather than actual usage, but with a system of credit rollovers.
For EBS-backed instances, the EBS volumes persist even when the instance is stopped. For long-running instances, the EBS costs are quite small compared to the instance costs. However, for instances that are run for only a few hours a day and stopped the rest of the time, the EBS volumes can be a significant fraction of the overall costs.
Depending on the settings used when launching the EBS, the EBS volume may or may not persist after the instance is terminated. If the volume persists, then that can cause significant cost leakage if the EBS volumes are not cleared out.
If you frequently provision new instances and do not automatically terminate the EBS upon termination of the instance, EBS can cause significant cost leakage.
 An EBS snapshot stores a snapshot of the current content of the EBS to S3.Whereas an EBS is tied to an availability zone, the EBS snapshot is available throughout the region, so it can be retrieved in any availability zone within the region. It can also be transferred across regions.
Although EBS snapshots are stored in S3, the metadata to retrieve them is stored in the EBS system. They cannot be accessed directly as S3 objects. Thus, even though the underlying data is stored in a highly redundant fashion, the snapshots enjoy only 99.9% reliability (as opposed to 99.99%+ for S3).
EBS snapshots can be transferred across regions. The usual charges for cross-region data transfer apply.Storage for EBS snapshots is incremental, so that if an EBS is snapshotted multiple times, only the changed contents between snapshots are stored. The deletion process, however, is smart and reconstructs later snapshots before deleting earlier ones.,
On-demand instances may be stopped and restarted at any time. The instance is not charged for while it is stopped. The local storage (if any) of the instance is destroyed and any public IP associated with the instance is freed (unless it was an elastic IP). However, the Elastic Block Storage (EBS) associated with the instance is preserved, and AWS still charges for it.
On-demand instances may be terminated by the user at any time. After the on-demand instance is terminated, the corresponding Elastic Block Storage may or may not be deleted. This depends on the settings specified at launch.
AWS will not stop or terminate on-demand instances, though the instances may occasionally become unavailable due to hardware degradation or other data center issues.
On-demand instances are also eligible for termination protection that makes it a little harder for the user to accidentally terminate the instance.


At the time of creation, the user creating the instance specifies the maximum spot price in addition to specifying the availability zone and instance type.
As long as the current spot price for that availability zone, and instance type is less than the maximum spot price, the instance can get created and will not get terminated. However, as soon as the current price exceeds the price of the spot instance, the instance gets terminated.
The price that is actually charged per unit time is the current spot price rather than the maximum spot price.
Spot instances cannot be stopped. They can only be terminated, either by the user or by AWS for price reasons.
The spot price for a spot instance cannot be changed after the instance has been created.
Spot instance pricing history by region, availability zone, and instance type is available on Amazon, and can be used to make smart bidding decisions.
There are limits on the number of spot instances that can be created by a given user for a given instance type and availability zone. These limits are generally much tighter than the limits associated with the overall number of instances, because of the havoc people can create by irresponsibly spinning up spot instances with high spot prices (and causing overall prices to spike). However, these limits can generally be increased upon request subject to capacity being available.For some instance types and availability zones, particularly the I instance type, spinning up spot instances can take a lot of time due to the low overall spot capacity, despite a nominally low spot price.
 These include the time period for the reservation, the type of payment plan, the operating system, the tenancy type (dedicated versus default), and the region.
For standard reserved instances (standard RIs), the instance class and generation (such as R3, C3, M3, M4), cannot be changed.
For standard RIs, you can change the instance size within the same instance class and generation. The instance size can be changed while keeping the overall capacity the same. For instance, a reservation for three m3.xlarge instances can be changed to a reservation for one m3.2xlarge instance and one m3.xlarge instance.
If your reservations have availability zone scope, you need to switch the availability zone or change to region scope to use the reservation in a different availability zone.
Note that resizing instances, getting region scope, or changing availability zone, is not possible for reservations tied to operating systems that have license costs, such as Windows operating systems.
The reservation is not tied to any particular instance. In fact, the instances to which reservations apply are created the same way as on-demand instances. The way reservations work is that at every hour when the billing is calculated, the existing on-demand instances being used are checked against the reservations currently active. If any of the reservations apply, then the reduced prices based on the reservations apply to the instances. Otherwise, the full on-demand price applies.
For convertible reserved instances (convertible RIs) the instance class and generation can be changed. If the new configuration costs more than the old one, you pay the difference, if it costs less, AWS does not refund you the difference, but you can sell the excess capacity in the reserved instance marketplace.
 Make this script sufficiently flexible that you can deploy both on-demand and spot instances with the same script.,, Autoscaling allows you to scale up instance capacity in real time in response to load increases. It is a little extra work to set up., To the extent possible, use S3 or databases for any long-lived data., They could handle updates in any of the following ways:

The applications themselves can be updated on a live production instance without needing to take that instance offline. While this may be true of some types of updates, you should not rely on this as the only way the application can be updated.
New instances with the updated application code are deployed, and connected to the load balancer, and the old instances are then disconnected from the load balancer and terminated. For this kind of update, capacity is temporarily larger during the update. Note that the excess instance capacity will fall outside the reserved capacity, and therefore the new instances, if on-demand, will be charged at full on-demand rates during the transition.
Each of the existing instances is updated. If the current production load can be handled by fewer than the full set of instances, then the instances can be updated one by one: each instance is disconnected from the load balancer, updated, and then reconnected to the load balancer. For this kind of update, capacity is temporarily smaller during the update. If production loads vary by time of day, this kind of update can be done at a time when the production load is low.
, Any availability zone in a given region can be activated for an ELB tied to that region.,,

Every instance (whether on-demand or spot) is billed in discrete units of hours. The hourly cycle for an instance begins when it is first provisioned. An instance is billed for a full hour even if it is used for part of an hour. For instance, if an instance is used for 15 minutes, it is charged for an hour. If it is used for 65 minutes, it is charged for 2 hours.
The hours for each instance are calculated separately, so if two instances are each used for 5 minutes they will be charged for two hours.If an instance is stopped and then restarted, a new hour is started every time the instance is restarted. So, if you stop and restart an instance four times in quick succession, you effectively get charged for four extra hours.If an Amazon EC2 instance is terminated by Amazon (rather than by the user) it is not charged for the hour during which it is terminated. This is particularly relevant for spot instances that are subject to termination due to price.
 You can break down the information by region, availability zone, instance class, instance type, and purchase option, and look at the usage on an hourly or daily basis. Data does not flow in immediately and can be delayed by 24-48 hours., Set up a billing alert so that the data starts getting sent to Amazon CloudWatch. You can then set up more alerts using CloudWatch.CloudWatch data comes in as data points every few hours, but does not include a detailed breakdown., This data is usually up to 6 hours late, though it can be even more delayed for some services., You need to figure out the mix of instances you'll use by instance class, instance type, region, availability zone, and purchase option., There should be no unreserved on-demand capacity except very temporarily when spinning up new instances to replace existing ones.

However, since reservations entail a long-term commitment, it might make sense to use on-demand instances instead for mission-critical applications where the details of the instance types and capacity needed are still unclear.
In general, reservations give the most saving for more exotic instances (such as the D, I, or P instances) but are also the riskiest for these since these instances have very specific use cases where they are valuable.
 Make sure that costs are part of the data you are looking at on a regular basis. Revisit your capacity decisions based on what you keep discovering.